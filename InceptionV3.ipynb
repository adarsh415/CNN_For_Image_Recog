{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "InceptionV3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOEJHb9XoNayMBHnKH8yCZx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adarsh415/CNN_For_Image_Recog/blob/master/InceptionV3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9vGLv3-oGWK",
        "colab_type": "code",
        "outputId": "42ba273d-5581-43de-c0ed-f2c73931f43f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as k\n",
        "from tensorflow.keras import metrics, datasets, layers, models, optimizers\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3nY3S9boqXH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data():\n",
        "  (Xtrain, Ytrain), (Xtest, Ytest) = datasets.cifar10.load_data()\n",
        "  Xtrain, Xtest = Xtrain.astype(np.float32)/255.0, Xtest.astype(np.float32)/255.0\n",
        "  trainDS = tf.data.Dataset.from_tensor_slices((Xtrain, Ytrain))\n",
        "  trainDS = trainDS.shuffle(50000).batch(128)\n",
        "  testDS = tf.data.Dataset.from_tensor_slices((Xtest, Ytest))\n",
        "  testDS = testDS.batch(64)\n",
        "\n",
        "  return trainDS, testDS"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVND_HO8po68",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InceptionBlk(models.Model):\n",
        "  def __init__(self,\n",
        "               filter_1x1,\n",
        "               filter_3x3,\n",
        "               reduce_3x3,\n",
        "               filter_5x5,\n",
        "               reduce_5x5,\n",
        "               pool_filter,\n",
        "               kernel_init, bias_init):\n",
        "    super(InceptionBlk, self).__init__(name='InceptionBlk')\n",
        "\n",
        "    self.conv1x1 = layers.Conv2D(filter_1x1, kernel_size=1, strides = 1, padding='same',activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)\n",
        "    self.conv3x3_reduce =  layers.Conv2D(reduce_3x3, kernel_size=1, strides = 1, padding='same',activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)\n",
        "    self.conv3x3 = layers.Conv2D(filter_3x3, kernel_size=3, strides = 1, padding='same',activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)\n",
        "    self.conv5x5_reduce = layers.Conv2D(reduce_5x5,kernel_size=1, strides = 1, padding='same',activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)\n",
        "    self.conv5x5 = layers.Conv2D(filter_5x5, kernel_size=3, strides = 1, padding='same',activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)\n",
        "    self.max5x5 = layers.MaxPool2D(pool_size=(5,5), strides=1, padding='same')\n",
        "    self.conv1x1_max = layers.Conv2D(pool_filter, kernel_size=1, strides = 1, padding='same',activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)\n",
        "\n",
        "  def call(self, x, training=None):\n",
        "\n",
        "    x_1 = self.conv1x1(x)\n",
        "\n",
        "    x_3 = self.conv3x3_reduce(x)\n",
        "    x_3 = self.conv3x3(x_3)\n",
        "\n",
        "    x_5 = self.conv5x5_reduce(x)\n",
        "    x_5 = self.conv5x5(x_5)\n",
        "\n",
        "    x_pool = self.max5x5(x)\n",
        "    x_pool = self.conv1x1_max(x_pool)\n",
        "\n",
        "    return layers.concatenate([x_1, x_3, x_5, x_pool])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lc3Ytgxb00HM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InceptionV3(models.Model):\n",
        "  def __init__(self, kernel_init, bias_init, aux=3):\n",
        "    super(InceptionV3, self).__init__(self)\n",
        "    self.kernel_init = kernel_init\n",
        "    self.bias_init = bias_init\n",
        "    self.aux = aux\n",
        "\n",
        "\n",
        "  def call(self, x, training=None):\n",
        "\n",
        "    # first conv block\n",
        "    block_1 = layers.Conv2D(64, kernel_size=7, strides=2, padding='same', activation='relu', kernel_initializer=self.kernel_init, bias_initializer=self.bias_init)(x)\n",
        "    block_1 = layers.MaxPool2D(pool_size=3, strides=2, padding='same')(block_1)\n",
        "    block_1 = layers.BatchNormalization()(block_1)\n",
        "\n",
        "    # second conv block\n",
        "    block_2 = layers.Conv2D(64, kernel_size=1, strides=1, padding='same', activation='relu', kernel_initializer=self.kernel_init, bias_initializer=self.bias_init)(x)\n",
        "    block_2 = layers.Conv2D(192, kernel_size=3, strides=1, padding='same', activation='relu', kernel_initializer=self.kernel_init, bias_initializer=self.bias_init)(x)\n",
        "    block_2 = layers.BatchNormalization()(block_2)\n",
        "    block_2 = layers.MaxPool2D(pool_size=3, strides=2, padding='same')(block_2)\n",
        "\n",
        "    # inception block 1a\n",
        "    inception_1a = InceptionBlk(filter_1x1=64,\n",
        "                                filter_3x3=128,\n",
        "                                reduce_3x3=96,\n",
        "                                filter_5x5=32,\n",
        "                                reduce_5x5=16,\n",
        "                                pool_filter=32,\n",
        "                                kernel_init=self.kernel_init,\n",
        "                                bias_init= self.bias_init)(block_2)\n",
        "    # inception block 2a\n",
        "    inception_2a = InceptionBlk(filter_1x1=128,\n",
        "                                filter_3x3=192,\n",
        "                                reduce_3x3=128,\n",
        "                                filter_5x5=96,\n",
        "                                reduce_5x5=32,\n",
        "                                pool_filter=64,\n",
        "                                kernel_init=self.kernel_init,\n",
        "                                bias_init= self.bias_init)(inception_1a)\n",
        "\n",
        "    # max pool layer\n",
        "    pool_1 = layers.MaxPool2D(pool_size=3, strides=2, padding='same')(inception_2a)\n",
        "\n",
        "    # inception block 3a\n",
        "    inception_3a = InceptionBlk(filter_1x1=192,\n",
        "                                filter_3x3=208,\n",
        "                                reduce_3x3=96,\n",
        "                                filter_5x5=48,\n",
        "                                reduce_5x5=16,\n",
        "                                pool_filter=64,\n",
        "                                kernel_init=self.kernel_init,\n",
        "                                bias_init= self.bias_init)(pool_1)\n",
        "\n",
        "    # first softmax layer\n",
        "    soft_1 = layers.AveragePooling2D(pool_size=5, strides=3, padding='valid')(inception_3a)\n",
        "    soft_1 = layers.Conv2D(128, kernel_size=1, strides=1, padding='same', activation='relu', kernel_initializer=self.kernel_init, bias_initializer=self.bias_init)(soft_1)\n",
        "    soft_1 = layers.Flatten()(soft_1)\n",
        "    soft_1 = layers.Dense(1024, activation='relu')(soft_1)\n",
        "    soft_1 = layers.Dropout(0.7)(soft_1)\n",
        "    soft_1 = layers.Dense(10, activation='softmax')(soft_1)\n",
        "    \n",
        "\n",
        "    #inception block 4a\n",
        "    inception_4a = InceptionBlk(filter_1x1=160,\n",
        "                                filter_3x3=224,\n",
        "                                reduce_3x3=112,\n",
        "                                filter_5x5=64,\n",
        "                                reduce_5x5=24,\n",
        "                                pool_filter=64,\n",
        "                                kernel_init=self.kernel_init,\n",
        "                                bias_init= self.bias_init)(inception_3a)\n",
        "\n",
        "    #inception block 5a\n",
        "    inception_5a = InceptionBlk(filter_1x1=128,\n",
        "                                filter_3x3=256,\n",
        "                                reduce_3x3=128,\n",
        "                                filter_5x5=64,\n",
        "                                reduce_5x5=24,\n",
        "                                pool_filter=64,\n",
        "                                kernel_init=self.kernel_init,\n",
        "                                bias_init= self.bias_init)(inception_4a)\n",
        "\n",
        "    #inception block 6a\n",
        "    inception_6a = InceptionBlk(filter_1x1=112,\n",
        "                                filter_3x3=288,\n",
        "                                reduce_3x3=144,\n",
        "                                filter_5x5=64,\n",
        "                                reduce_5x5=32,\n",
        "                                pool_filter=64,\n",
        "                                kernel_init=self.kernel_init,\n",
        "                                bias_init= self.bias_init)(inception_5a)\n",
        "\n",
        "\n",
        "    #second softmax layer\n",
        "    soft_2 = layers.AveragePooling2D(pool_size=5, strides=3, padding='valid')(inception_6a)\n",
        "    soft_2 = layers.Conv2D(128, kernel_size=1, strides=1, padding='same', activation='relu', kernel_initializer=self.kernel_init, bias_initializer=self.bias_init)(soft_2)\n",
        "    soft_2 = layers.Flatten()(soft_2)\n",
        "    soft_2 = layers.Dense(1024, activation='relu')(soft_2)\n",
        "    soft_2 = layers.Dropout(0.7)(soft_2)\n",
        "    soft_2 = layers.Dense(10, activation='softmax')(soft_2)\n",
        "\n",
        "    #inception block 7a\n",
        "    inception_7a = InceptionBlk(filter_1x1=256,\n",
        "                                filter_3x3=320,\n",
        "                                reduce_3x3=160,\n",
        "                                filter_5x5=128,\n",
        "                                reduce_5x5=32,\n",
        "                                pool_filter=128,\n",
        "                                kernel_init=self.kernel_init,\n",
        "                                bias_init= self.bias_init)(inception_6a)\n",
        "\n",
        "    # max pool layer\n",
        "    pool_2 = layers.MaxPool2D(pool_size=3, strides=2, padding='same')(inception_7a)\n",
        "    \n",
        "    #inception block 8a\n",
        "    inception_8a = InceptionBlk(filter_1x1=256,\n",
        "                                filter_3x3=320,\n",
        "                                reduce_3x3=160,\n",
        "                                filter_5x5=128,\n",
        "                                reduce_5x5=32,\n",
        "                                pool_filter=128,\n",
        "                                kernel_init=self.kernel_init,\n",
        "                                bias_init= self.bias_init)(pool_2)\n",
        "    \n",
        "    #inception block 9a\n",
        "    inception_9a = InceptionBlk(filter_1x1=384,\n",
        "                                filter_3x3=384,\n",
        "                                reduce_3x3=192,\n",
        "                                filter_5x5=128,\n",
        "                                reduce_5x5=48,\n",
        "                                pool_filter=128,\n",
        "                                kernel_init=self.kernel_init,\n",
        "                                bias_init= self.bias_init)(inception_8a)\n",
        "\n",
        "    #third softmax layer\n",
        "    #soft_3 = layers.AveragePooling2D(pool_size=7, strides=1, padding='valid')(inception_9a)\n",
        "    #soft_3 = layers.AveragePooling2D()(inception_9a)\n",
        "    soft_3 = layers.GlobalAveragePooling2D()(inception_9a)\n",
        "    soft_3 = layers.Dropout(0.4)(soft_3)\n",
        "    soft_3 = layers.Flatten()(soft_3)\n",
        "    soft_3 = layers.Dense(1024, activation='relu')(soft_3)\n",
        "    soft_3 = layers.Dense(10, activation='softmax')(soft_3)\n",
        "\n",
        "    if self.aux == 1:\n",
        "      return soft_1\n",
        "    elif self.aux == 2:\n",
        "      return soft_2\n",
        "    else:\n",
        "    #return layers.concatenate([soft_1,soft_2, soft_3], axis=-1)\n",
        "      return soft_3\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XY-5iOncEfKX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train():\n",
        "    n_epoch = 50\n",
        "    train_ds, test_ds = load_data()\n",
        "\n",
        "    optimizer = optimizers.Adam(learning_rate=1e-3)\n",
        "\n",
        "    kernel_init = k.initializers.he_normal()\n",
        "    bias_init = k.initializers.he_normal()\n",
        "    model1, model2, model3 = InceptionV3(kernel_init,bias_init,aux=1), InceptionV3(kernel_init,bias_init,aux=2), InceptionV3(kernel_init,bias_init)\n",
        "\n",
        "    # acc = metrics.CategoricalAccuracy()\n",
        "    acc = metrics.Accuracy()\n",
        "    lossfn = tf.keras.losses.CategoricalCrossentropy()\n",
        "    \n",
        "    for epoch in range(n_epoch):\n",
        "        losses = 0.0\n",
        "        accuracy = 0.0\n",
        "        #step = 0\n",
        "        for step, (x,y) in enumerate(train_ds):\n",
        "            #print(.shape)\n",
        "            #step += 1\n",
        "            # with tf.GradientTape() as tape1:\n",
        "            #     logits1 = model1(x)\n",
        "            #     loss1 = lossfn(tf.one_hot(y,10),logits1)\n",
        "                \n",
        "            # with tf.GradientTape() as tape2:\n",
        "            #   logits2 = model2(x)\n",
        "            #   loss2 = lossfn(tf.one_hot(y,10),logits2)\n",
        "\n",
        "            with tf.GradientTape() as tape3:\n",
        "              logits3 = model3(x)\n",
        "              #print('logit shape', logits3.shape)\n",
        "              loss3 = lossfn(tf.one_hot(y,10),logits3)\n",
        "              #acc.update_state(tf.one_hot(y,10), logits3) this should be used with CategoricalAccuracy()\n",
        "              acc.update_state(y, tf.argmax(logits3, axis=1, output_type=tf.int32))\n",
        "              losses = loss3\n",
        "\n",
        "            # grads1 = tape1.gradient(loss1, model1.trainable_variables)\n",
        "            # grads1 = [tf.clip_by_norm(g, 15) for g in grads1]\n",
        "            # optimizer.apply_gradients(zip(grads1, model1.trainable_variables))\n",
        "\n",
        "            # grads2 = tape2.gradient(loss2, model2.trainable_variables)\n",
        "            # grads2 = [tf.clip_by_norm(g, 15) for g in grads2]\n",
        "            # optimizer.apply_gradients(zip(grads2, model2.trainable_variables))\n",
        "\n",
        "            grads3 = tape3.gradient(loss3, model3.trainable_variables)\n",
        "            grads3 = [tf.clip_by_norm(g, 15) for g in grads3]\n",
        "            optimizer.apply_gradients(zip(grads3, model3.trainable_variables))\n",
        "\n",
        "            \n",
        "            if step%1000 == 0:\n",
        "                #accuracy = compute_accuracy(logits, tf.cast(y, tf.int64))\n",
        "                #print(f' accuracy in epoch {epoch} after steps {step} is: {accuracy.numpy()}')\n",
        "                print(f' accuracy in epoch {epoch} after steps {step} is: {acc.result().numpy()}')\n",
        "                acc.reset_states()\n",
        "                #print(f' Loss in epoch {epoch} after steps {step} is: {losses}')\n",
        "                print(f' Loss for main model3 in epoch {epoch} after steps {step} is: {losses.numpy()}')\n",
        "                # print(f' Loss for auxialiary model2 in epoch {epoch} after steps {step} is: {loss2.numpy()}')\n",
        "                # print(f' Loss for auxialiary model2 in epoch {epoch} after steps {step} is: {loss1.numpy()}')\n",
        "\n",
        "        print(f'loss after epoch {epoch} is {losses.numpy()}')\n",
        "\n",
        "\n",
        "\n",
        "    print(f'final training accuracy is {acc.result().numpy()}')\n",
        "    acc.reset_states()\n",
        "\n",
        "\n",
        "    #model.save('VGG16', save_format='tf')\n",
        "    for step, (x, y) in enumerate(test_ds):\n",
        "        logits = model3(x, training=False)\n",
        "        acc.update_state(y, tf.argmax(logits, axis=1, output_type=tf.int32))\n",
        "        #acc.update_state(tf.one_hot(y,10), logits)\n",
        "        # if step%1000 == 0:\n",
        "        #     #y = tf.squeeze(y, axis=1)\n",
        "        #     #accuracy = compute_accuracy(logits,tf.cast(y, tf.int64))\n",
        "            \n",
        "    print(f'test accuracy is {acc.result().numpy()}')\n",
        "    acc.reset_states()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3sU-4p6Fu9Z",
        "colab_type": "code",
        "outputId": "02e14ce2-807c-4c1f-b62c-195cb4021444",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "source": [
        "train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " accuracy in epoch 0 after steps 0 is: 0.0703125\n",
            " Loss for main model3 in epoch 0 after steps 0 is: 3.434532880783081\n",
            "loss after epoch 0 is 2.5541484355926514\n",
            " accuracy in epoch 1 after steps 0 is: 0.10106000304222107\n",
            " Loss for main model3 in epoch 1 after steps 0 is: 2.5021181106567383\n",
            "loss after epoch 1 is 3.022416353225708\n",
            " accuracy in epoch 2 after steps 0 is: 0.09882000088691711\n",
            " Loss for main model3 in epoch 2 after steps 0 is: 2.5204873085021973\n",
            "loss after epoch 2 is 2.522526741027832\n",
            " accuracy in epoch 3 after steps 0 is: 0.09892000257968903\n",
            " Loss for main model3 in epoch 3 after steps 0 is: 2.7066874504089355\n",
            "loss after epoch 3 is 2.4204823970794678\n",
            " accuracy in epoch 4 after steps 0 is: 0.09950000047683716\n",
            " Loss for main model3 in epoch 4 after steps 0 is: 3.10794734954834\n",
            "loss after epoch 4 is 2.690838575363159\n",
            " accuracy in epoch 5 after steps 0 is: 0.10062000155448914\n",
            " Loss for main model3 in epoch 5 after steps 0 is: 2.4790072441101074\n",
            "loss after epoch 5 is 2.8630917072296143\n",
            " accuracy in epoch 6 after steps 0 is: 0.100040003657341\n",
            " Loss for main model3 in epoch 6 after steps 0 is: 2.5096073150634766\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxxBxZjlFxSX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}