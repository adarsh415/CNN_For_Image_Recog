{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "InceptionV3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOEJHb9XoNayMBHnKH8yCZx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adarsh415/CNN_For_Image_Recog/blob/master/InceptionV3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9vGLv3-oGWK",
        "colab_type": "code",
        "outputId": "69646dcf-436a-490e-dc71-5fe75f208d37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as k\n",
        "from tensorflow.keras import metrics, datasets, layers, models, optimizers\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3nY3S9boqXH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data():\n",
        "  (Xtrain, Ytrain), (Xtest, Ytest) = datasets.cifar10.load_data()\n",
        "  Xtrain, Xtest = Xtrain.astype(np.float32)/255.0, Xtest.astype(np.float32)/255.0\n",
        "  trainDS = tf.data.Dataset.from_tensor_slices((Xtrain, Ytrain))\n",
        "  trainDS = trainDS.shuffle(50000).batch(128)\n",
        "  testDS = tf.data.Dataset.from_tensor_slices((Xtest, Ytest))\n",
        "  testDS = testDS.batch(64)\n",
        "\n",
        "  return trainDS, testDS"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVND_HO8po68",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InceptionBlk(models.Model):\n",
        "  def __init__(self,\n",
        "               filter_1x1,\n",
        "               filter_3x3,\n",
        "               reduce_3x3,\n",
        "               filter_5x5,\n",
        "               reduce_5x5,\n",
        "               pool_filter,\n",
        "               kernel_init, bias_init):\n",
        "    super(InceptionBlk, self).__init__(name='InceptionBlk')\n",
        "\n",
        "    self.conv1x1 = layers.Conv2D(filter_1x1, kernel_size=1, strides = 1, padding='same',activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)\n",
        "    self.conv3x3_reduce =  layers.Conv2D(reduce_3x3, kernel_size=1, strides = 1, padding='same',activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)\n",
        "    self.conv3x3 = layers.Conv2D(filter_3x3, kernel_size=3, strides = 1, padding='same',activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)\n",
        "    self.conv5x5_reduce = layers.Conv2D(reduce_5x5,kernel_size=1, strides = 1, padding='same',activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)\n",
        "    self.conv5x5 = layers.Conv2D(filter_5x5, kernel_size=3, strides = 1, padding='same',activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)\n",
        "    self.max5x5 = layers.MaxPool2D(pool_size=(5,5), strides=1, padding='same')\n",
        "    self.conv1x1_max = layers.Conv2D(pool_filter, kernel_size=1, strides = 1, padding='same',activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)\n",
        "\n",
        "  def call(self, x, training=None):\n",
        "\n",
        "    x_1 = self.conv1x1(x)\n",
        "\n",
        "    x_3 = self.conv3x3_reduce(x)\n",
        "    x_3 = self.conv3x3(x_3)\n",
        "\n",
        "    x_5 = self.conv5x5_reduce(x)\n",
        "    x_5 = self.conv5x5(x_5)\n",
        "\n",
        "    x_pool = self.max5x5(x)\n",
        "    x_pool = self.conv1x1_max(x_pool)\n",
        "\n",
        "    return layers.concatenate([x_1, x_3, x_5, x_pool])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lc3Ytgxb00HM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InceptionV3(models.Model):\n",
        "  def __init__(self, kernel_init, bias_init, aux=3):\n",
        "    super(InceptionV3, self).__init__(self)\n",
        "    self.kernel_init = kernel_init\n",
        "    self.bias_init = bias_init\n",
        "    self.aux = aux\n",
        "\n",
        "\n",
        "  def call(self, x, training=None):\n",
        "\n",
        "    # first conv block\n",
        "    block_1 = layers.Conv2D(64, kernel_size=7, strides=2, padding='same', activation='relu', kernel_initializer=self.kernel_init, bias_initializer=self.bias_init)(x)\n",
        "    block_1 = layers.MaxPool2D(pool_size=3, strides=2, padding='same')(block_1)\n",
        "    block_1 = layers.BatchNormalization()(block_1)\n",
        "\n",
        "    # second conv block\n",
        "    block_2 = layers.Conv2D(64, kernel_size=1, strides=1, padding='same', activation='relu', kernel_initializer=self.kernel_init, bias_initializer=self.bias_init)(x)\n",
        "    block_2 = layers.Conv2D(192, kernel_size=3, strides=1, padding='same', activation='relu', kernel_initializer=self.kernel_init, bias_initializer=self.bias_init)(x)\n",
        "    block_2 = layers.BatchNormalization()(block_2)\n",
        "    block_2 = layers.MaxPool2D(pool_size=3, strides=2, padding='same')(block_2)\n",
        "\n",
        "    # inception block 1a\n",
        "    inception_1a = InceptionBlk(filter_1x1=64,\n",
        "                                filter_3x3=128,\n",
        "                                reduce_3x3=96,\n",
        "                                filter_5x5=32,\n",
        "                                reduce_5x5=16,\n",
        "                                pool_filter=32,\n",
        "                                kernel_init=self.kernel_init,\n",
        "                                bias_init= self.bias_init)(block_2)\n",
        "    # inception block 2a\n",
        "    inception_2a = InceptionBlk(filter_1x1=128,\n",
        "                                filter_3x3=192,\n",
        "                                reduce_3x3=128,\n",
        "                                filter_5x5=96,\n",
        "                                reduce_5x5=32,\n",
        "                                pool_filter=64,\n",
        "                                kernel_init=self.kernel_init,\n",
        "                                bias_init= self.bias_init)(inception_1a)\n",
        "\n",
        "    # max pool layer\n",
        "    pool_1 = layers.MaxPool2D(pool_size=3, strides=2, padding='same')(inception_2a)\n",
        "\n",
        "    # inception block 3a\n",
        "    inception_3a = InceptionBlk(filter_1x1=192,\n",
        "                                filter_3x3=208,\n",
        "                                reduce_3x3=96,\n",
        "                                filter_5x5=48,\n",
        "                                reduce_5x5=16,\n",
        "                                pool_filter=64,\n",
        "                                kernel_init=self.kernel_init,\n",
        "                                bias_init= self.bias_init)(pool_1)\n",
        "\n",
        "    # first softmax layer\n",
        "    soft_1 = layers.AveragePooling2D(pool_size=5, strides=3, padding='valid')(inception_3a)\n",
        "    soft_1 = layers.Conv2D(128, kernel_size=1, strides=1, padding='same', activation='relu', kernel_initializer=self.kernel_init, bias_initializer=self.bias_init)(soft_1)\n",
        "    soft_1 = layers.Flatten()(soft_1)\n",
        "    soft_1 = layers.Dense(1024, activation='relu')(soft_1)\n",
        "    soft_1 = layers.Dropout(0.7)(soft_1)\n",
        "    soft_1 = layers.Dense(10, activation='softmax')(soft_1)\n",
        "    \n",
        "\n",
        "    #inception block 4a\n",
        "    inception_4a = InceptionBlk(filter_1x1=160,\n",
        "                                filter_3x3=224,\n",
        "                                reduce_3x3=112,\n",
        "                                filter_5x5=64,\n",
        "                                reduce_5x5=24,\n",
        "                                pool_filter=64,\n",
        "                                kernel_init=self.kernel_init,\n",
        "                                bias_init= self.bias_init)(inception_3a)\n",
        "\n",
        "    #inception block 5a\n",
        "    inception_5a = InceptionBlk(filter_1x1=128,\n",
        "                                filter_3x3=256,\n",
        "                                reduce_3x3=128,\n",
        "                                filter_5x5=64,\n",
        "                                reduce_5x5=24,\n",
        "                                pool_filter=64,\n",
        "                                kernel_init=self.kernel_init,\n",
        "                                bias_init= self.bias_init)(inception_4a)\n",
        "\n",
        "    #inception block 6a\n",
        "    inception_6a = InceptionBlk(filter_1x1=112,\n",
        "                                filter_3x3=288,\n",
        "                                reduce_3x3=144,\n",
        "                                filter_5x5=64,\n",
        "                                reduce_5x5=32,\n",
        "                                pool_filter=64,\n",
        "                                kernel_init=self.kernel_init,\n",
        "                                bias_init= self.bias_init)(inception_5a)\n",
        "\n",
        "\n",
        "    #second softmax layer\n",
        "    soft_2 = layers.AveragePooling2D(pool_size=5, strides=3, padding='valid')(inception_6a)\n",
        "    soft_2 = layers.Conv2D(128, kernel_size=1, strides=1, padding='same', activation='relu', kernel_initializer=self.kernel_init, bias_initializer=self.bias_init)(soft_2)\n",
        "    soft_2 = layers.Flatten()(soft_2)\n",
        "    soft_2 = layers.Dense(1024, activation='relu')(soft_2)\n",
        "    soft_2 = layers.Dropout(0.7)(soft_2)\n",
        "    soft_2 = layers.Dense(10, activation='softmax')(soft_2)\n",
        "\n",
        "    #inception block 7a\n",
        "    inception_7a = InceptionBlk(filter_1x1=256,\n",
        "                                filter_3x3=320,\n",
        "                                reduce_3x3=160,\n",
        "                                filter_5x5=128,\n",
        "                                reduce_5x5=32,\n",
        "                                pool_filter=128,\n",
        "                                kernel_init=self.kernel_init,\n",
        "                                bias_init= self.bias_init)(inception_6a)\n",
        "\n",
        "    # max pool layer\n",
        "    pool_2 = layers.MaxPool2D(pool_size=3, strides=2, padding='same')(inception_7a)\n",
        "    \n",
        "    #inception block 8a\n",
        "    inception_8a = InceptionBlk(filter_1x1=256,\n",
        "                                filter_3x3=320,\n",
        "                                reduce_3x3=160,\n",
        "                                filter_5x5=128,\n",
        "                                reduce_5x5=32,\n",
        "                                pool_filter=128,\n",
        "                                kernel_init=self.kernel_init,\n",
        "                                bias_init= self.bias_init)(pool_2)\n",
        "    \n",
        "    #inception block 9a\n",
        "    inception_9a = InceptionBlk(filter_1x1=384,\n",
        "                                filter_3x3=384,\n",
        "                                reduce_3x3=192,\n",
        "                                filter_5x5=128,\n",
        "                                reduce_5x5=48,\n",
        "                                pool_filter=128,\n",
        "                                kernel_init=self.kernel_init,\n",
        "                                bias_init= self.bias_init)(inception_8a)\n",
        "\n",
        "    #third softmax layer\n",
        "    #soft_3 = layers.AveragePooling2D(pool_size=7, strides=1, padding='valid')(inception_9a)\n",
        "    #soft_3 = layers.AveragePooling2D()(inception_9a)\n",
        "    soft_3 = layers.GlobalAveragePooling2D()(inception_9a)\n",
        "    soft_3 = layers.Dropout(0.4)(soft_3)\n",
        "    soft_3 = layers.Flatten()(soft_3)\n",
        "    soft_3 = layers.Dense(1024, activation='relu')(soft_3)\n",
        "    soft_3 = layers.Dense(10, activation='softmax')(soft_3)\n",
        "\n",
        "    if self.aux == 1:\n",
        "      return soft_1\n",
        "    elif self.aux == 2:\n",
        "      return soft_2\n",
        "    else:\n",
        "    #return layers.concatenate([soft_1,soft_2, soft_3], axis=-1)\n",
        "      return soft_3\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XY-5iOncEfKX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train():\n",
        "    n_epoch = 50\n",
        "    train_ds, test_ds = load_data()\n",
        "\n",
        "    optimizer = optimizers.Adam(learning_rate=1e-3)\n",
        "\n",
        "    kernel_init = k.initializers.he_normal()\n",
        "    bias_init = k.initializers.he_normal()\n",
        "    model1, model2, model3 = InceptionV3(kernel_init,bias_init,aux=1), InceptionV3(kernel_init,bias_init,aux=2), InceptionV3(kernel_init,bias_init)\n",
        "\n",
        "    # acc = metrics.CategoricalAccuracy()\n",
        "    acc = metrics.Accuracy()\n",
        "    lossfn = tf.keras.losses.CategoricalCrossentropy()\n",
        "    \n",
        "    for epoch in range(n_epoch):\n",
        "        losses = 0.0\n",
        "        accuracy = 0.0\n",
        "        #step = 0\n",
        "        for step, (x,y) in enumerate(train_ds):\n",
        "            #print(.shape)\n",
        "            #step += 1\n",
        "            # with tf.GradientTape() as tape1:\n",
        "            #     logits1 = model1(x)\n",
        "            #     loss1 = lossfn(tf.one_hot(y,10),logits1)\n",
        "                \n",
        "            # with tf.GradientTape() as tape2:\n",
        "            #   logits2 = model2(x)\n",
        "            #   loss2 = lossfn(tf.one_hot(y,10),logits2)\n",
        "\n",
        "            with tf.GradientTape() as tape3:\n",
        "              logits3 = model3(x)\n",
        "              #print('logit shape', logits3.shape)\n",
        "              loss3 = lossfn(tf.one_hot(y,10),logits3)\n",
        "              #acc.update_state(tf.one_hot(y,10), logits3) this should be used with CategoricalAccuracy()\n",
        "              acc.update_state(y, tf.argmax(logits3, axis=1, output_type=tf.int32))\n",
        "              losses = loss3\n",
        "\n",
        "            # grads1 = tape1.gradient(loss1, model1.trainable_variables)\n",
        "            # grads1 = [tf.clip_by_norm(g, 15) for g in grads1]\n",
        "            # optimizer.apply_gradients(zip(grads1, model1.trainable_variables))\n",
        "\n",
        "            # grads2 = tape2.gradient(loss2, model2.trainable_variables)\n",
        "            # grads2 = [tf.clip_by_norm(g, 15) for g in grads2]\n",
        "            # optimizer.apply_gradients(zip(grads2, model2.trainable_variables))\n",
        "\n",
        "            grads3 = tape3.gradient(loss3, model3.trainable_variables)\n",
        "            grads3 = [tf.clip_by_norm(g, 15) for g in grads3]\n",
        "            optimizer.apply_gradients(zip(grads3, model3.trainable_variables))\n",
        "\n",
        "            \n",
        "            if step%1000 == 0:\n",
        "                #accuracy = compute_accuracy(logits, tf.cast(y, tf.int64))\n",
        "                #print(f' accuracy in epoch {epoch} after steps {step} is: {accuracy.numpy()}')\n",
        "                print(f' accuracy in epoch {epoch} after steps {step} is: {acc.result().numpy()}')\n",
        "                acc.reset_states()\n",
        "                #print(f' Loss in epoch {epoch} after steps {step} is: {losses}')\n",
        "                print(f' Loss for main model3 in epoch {epoch} after steps {step} is: {losses.numpy()}')\n",
        "                # print(f' Loss for auxialiary model2 in epoch {epoch} after steps {step} is: {loss2.numpy()}')\n",
        "                # print(f' Loss for auxialiary model2 in epoch {epoch} after steps {step} is: {loss1.numpy()}')\n",
        "\n",
        "        print(f'loss after epoch {epoch} is {losses.numpy()}')\n",
        "\n",
        "\n",
        "\n",
        "    print(f'final training accuracy is {acc.result().numpy()}')\n",
        "    acc.reset_states()\n",
        "\n",
        "\n",
        "    #model.save('VGG16', save_format='tf')\n",
        "    for step, (x, y) in enumerate(test_ds):\n",
        "        logits = model3(x, training=False)\n",
        "        acc.update_state(y, tf.argmax(logits, axis=1, output_type=tf.int32))\n",
        "        #acc.update_state(tf.one_hot(y,10), logits)\n",
        "        # if step%1000 == 0:\n",
        "        #     #y = tf.squeeze(y, axis=1)\n",
        "        #     #accuracy = compute_accuracy(logits,tf.cast(y, tf.int64))\n",
        "            \n",
        "    print(f'test accuracy is {acc.result().numpy()}')\n",
        "    acc.reset_states()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3sU-4p6Fu9Z",
        "colab_type": "code",
        "outputId": "52b80372-3410-42f1-ebf4-3298a3a8b749",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 6s 0us/step\n",
            " accuracy in epoch 0 after steps 0 is: 0.1484375\n",
            " Loss for main model3 in epoch 0 after steps 0 is: 2.6438777446746826\n",
            "loss after epoch 0 is 2.6934618949890137\n",
            " accuracy in epoch 1 after steps 0 is: 0.09852000325918198\n",
            " Loss for main model3 in epoch 1 after steps 0 is: 3.379930019378662\n",
            "loss after epoch 1 is 2.859830379486084\n",
            " accuracy in epoch 2 after steps 0 is: 0.10010000318288803\n",
            " Loss for main model3 in epoch 2 after steps 0 is: 3.0153212547302246\n",
            "loss after epoch 2 is 2.6033594608306885\n",
            " accuracy in epoch 3 after steps 0 is: 0.1014999970793724\n",
            " Loss for main model3 in epoch 3 after steps 0 is: 2.8082785606384277\n",
            "loss after epoch 3 is 2.7471392154693604\n",
            " accuracy in epoch 4 after steps 0 is: 0.09880000352859497\n",
            " Loss for main model3 in epoch 4 after steps 0 is: 2.9206490516662598\n",
            "loss after epoch 4 is 2.9444186687469482\n",
            " accuracy in epoch 5 after steps 0 is: 0.09892000257968903\n",
            " Loss for main model3 in epoch 5 after steps 0 is: 3.3562703132629395\n",
            "loss after epoch 5 is 2.6857683658599854\n",
            " accuracy in epoch 6 after steps 0 is: 0.10074000060558319\n",
            " Loss for main model3 in epoch 6 after steps 0 is: 2.6175737380981445\n",
            "loss after epoch 6 is 2.9202632904052734\n",
            " accuracy in epoch 7 after steps 0 is: 0.10034000128507614\n",
            " Loss for main model3 in epoch 7 after steps 0 is: 2.5828652381896973\n",
            "loss after epoch 7 is 2.9153497219085693\n",
            " accuracy in epoch 8 after steps 0 is: 0.10115999728441238\n",
            " Loss for main model3 in epoch 8 after steps 0 is: 2.9764199256896973\n",
            "loss after epoch 8 is 2.4941084384918213\n",
            " accuracy in epoch 9 after steps 0 is: 0.09781999886035919\n",
            " Loss for main model3 in epoch 9 after steps 0 is: 3.24507474899292\n",
            "loss after epoch 9 is 2.4400174617767334\n",
            " accuracy in epoch 10 after steps 0 is: 0.1023000031709671\n",
            " Loss for main model3 in epoch 10 after steps 0 is: 2.8630359172821045\n",
            "loss after epoch 10 is 2.5749616622924805\n",
            " accuracy in epoch 11 after steps 0 is: 0.10029999911785126\n",
            " Loss for main model3 in epoch 11 after steps 0 is: 2.5384039878845215\n",
            "loss after epoch 11 is 3.489556312561035\n",
            " accuracy in epoch 12 after steps 0 is: 0.09978000074625015\n",
            " Loss for main model3 in epoch 12 after steps 0 is: 3.0830416679382324\n",
            "loss after epoch 12 is 2.615767002105713\n",
            " accuracy in epoch 13 after steps 0 is: 0.10006000101566315\n",
            " Loss for main model3 in epoch 13 after steps 0 is: 2.6942548751831055\n",
            "loss after epoch 13 is 2.5329384803771973\n",
            " accuracy in epoch 14 after steps 0 is: 0.09910000115633011\n",
            " Loss for main model3 in epoch 14 after steps 0 is: 3.292853355407715\n",
            "loss after epoch 14 is 2.782183885574341\n",
            " accuracy in epoch 15 after steps 0 is: 0.1004600003361702\n",
            " Loss for main model3 in epoch 15 after steps 0 is: 3.248412609100342\n",
            "loss after epoch 15 is 2.5608911514282227\n",
            " accuracy in epoch 16 after steps 0 is: 0.09821999818086624\n",
            " Loss for main model3 in epoch 16 after steps 0 is: 2.9188923835754395\n",
            "loss after epoch 16 is 2.3805267810821533\n",
            " accuracy in epoch 17 after steps 0 is: 0.09796000272035599\n",
            " Loss for main model3 in epoch 17 after steps 0 is: 3.260971784591675\n",
            "loss after epoch 17 is 2.5580644607543945\n",
            " accuracy in epoch 18 after steps 0 is: 0.098860003054142\n",
            " Loss for main model3 in epoch 18 after steps 0 is: 2.5412116050720215\n",
            "loss after epoch 18 is 2.761651039123535\n",
            " accuracy in epoch 19 after steps 0 is: 0.10034000128507614\n",
            " Loss for main model3 in epoch 19 after steps 0 is: 2.9748191833496094\n",
            "loss after epoch 19 is 2.6483001708984375\n",
            " accuracy in epoch 20 after steps 0 is: 0.09950000047683716\n",
            " Loss for main model3 in epoch 20 after steps 0 is: 2.925837755203247\n",
            "loss after epoch 20 is 2.4898760318756104\n",
            " accuracy in epoch 21 after steps 0 is: 0.10006000101566315\n",
            " Loss for main model3 in epoch 21 after steps 0 is: 2.5810184478759766\n",
            "loss after epoch 21 is 2.665316104888916\n",
            " accuracy in epoch 22 after steps 0 is: 0.09781999886035919\n",
            " Loss for main model3 in epoch 22 after steps 0 is: 2.697453260421753\n",
            "loss after epoch 22 is 2.636974573135376\n",
            " accuracy in epoch 23 after steps 0 is: 0.09651999920606613\n",
            " Loss for main model3 in epoch 23 after steps 0 is: 3.017359972000122\n",
            "loss after epoch 23 is 3.111675500869751\n",
            " accuracy in epoch 24 after steps 0 is: 0.10267999768257141\n",
            " Loss for main model3 in epoch 24 after steps 0 is: 2.8383378982543945\n",
            "loss after epoch 24 is 2.6880223751068115\n",
            " accuracy in epoch 25 after steps 0 is: 0.09685999900102615\n",
            " Loss for main model3 in epoch 25 after steps 0 is: 2.633068799972534\n",
            "loss after epoch 25 is 2.6353931427001953\n",
            " accuracy in epoch 26 after steps 0 is: 0.1006999984383583\n",
            " Loss for main model3 in epoch 26 after steps 0 is: 2.5291988849639893\n",
            "loss after epoch 26 is 2.4484856128692627\n",
            " accuracy in epoch 27 after steps 0 is: 0.09914000332355499\n",
            " Loss for main model3 in epoch 27 after steps 0 is: 2.566592216491699\n",
            "loss after epoch 27 is 2.9711151123046875\n",
            " accuracy in epoch 28 after steps 0 is: 0.1011200025677681\n",
            " Loss for main model3 in epoch 28 after steps 0 is: 2.6482443809509277\n",
            "loss after epoch 28 is 2.909640312194824\n",
            " accuracy in epoch 29 after steps 0 is: 0.10267999768257141\n",
            " Loss for main model3 in epoch 29 after steps 0 is: 2.493377923965454\n",
            "loss after epoch 29 is 2.9273416996002197\n",
            " accuracy in epoch 30 after steps 0 is: 0.10217999666929245\n",
            " Loss for main model3 in epoch 30 after steps 0 is: 2.901060104370117\n",
            "loss after epoch 30 is 2.8757827281951904\n",
            " accuracy in epoch 31 after steps 0 is: 0.09914000332355499\n",
            " Loss for main model3 in epoch 31 after steps 0 is: 2.369993209838867\n",
            "loss after epoch 31 is 2.7492122650146484\n",
            " accuracy in epoch 32 after steps 0 is: 0.09746000170707703\n",
            " Loss for main model3 in epoch 32 after steps 0 is: 2.7542176246643066\n",
            "loss after epoch 32 is 2.554297685623169\n",
            " accuracy in epoch 33 after steps 0 is: 0.10010000318288803\n",
            " Loss for main model3 in epoch 33 after steps 0 is: 3.371211528778076\n",
            "loss after epoch 33 is 2.594386577606201\n",
            " accuracy in epoch 34 after steps 0 is: 0.09944000095129013\n",
            " Loss for main model3 in epoch 34 after steps 0 is: 4.018790245056152\n",
            "loss after epoch 34 is 2.574294328689575\n",
            " accuracy in epoch 35 after steps 0 is: 0.10013999789953232\n",
            " Loss for main model3 in epoch 35 after steps 0 is: 3.44199800491333\n",
            "loss after epoch 35 is 2.558422803878784\n",
            " accuracy in epoch 36 after steps 0 is: 0.10153999924659729\n",
            " Loss for main model3 in epoch 36 after steps 0 is: 2.5222854614257812\n",
            "loss after epoch 36 is 2.454099655151367\n",
            " accuracy in epoch 37 after steps 0 is: 0.10220000147819519\n",
            " Loss for main model3 in epoch 37 after steps 0 is: 2.9037678241729736\n",
            "loss after epoch 37 is 2.4560728073120117\n",
            " accuracy in epoch 38 after steps 0 is: 0.10080000013113022\n",
            " Loss for main model3 in epoch 38 after steps 0 is: 2.9272475242614746\n",
            "loss after epoch 38 is 2.9757089614868164\n",
            " accuracy in epoch 39 after steps 0 is: 0.10074000060558319\n",
            " Loss for main model3 in epoch 39 after steps 0 is: 3.1177682876586914\n",
            "loss after epoch 39 is 2.5369579792022705\n",
            " accuracy in epoch 40 after steps 0 is: 0.09960000216960907\n",
            " Loss for main model3 in epoch 40 after steps 0 is: 2.753260612487793\n",
            "loss after epoch 40 is 2.693652629852295\n",
            " accuracy in epoch 41 after steps 0 is: 0.09907999634742737\n",
            " Loss for main model3 in epoch 41 after steps 0 is: 2.4639840126037598\n",
            "loss after epoch 41 is 3.0327863693237305\n",
            " accuracy in epoch 42 after steps 0 is: 0.09845999628305435\n",
            " Loss for main model3 in epoch 42 after steps 0 is: 2.578036308288574\n",
            "loss after epoch 42 is 2.4453399181365967\n",
            " accuracy in epoch 43 after steps 0 is: 0.09917999804019928\n",
            " Loss for main model3 in epoch 43 after steps 0 is: 3.206746816635132\n",
            "loss after epoch 43 is 2.453289270401001\n",
            " accuracy in epoch 44 after steps 0 is: 0.10162000358104706\n",
            " Loss for main model3 in epoch 44 after steps 0 is: 3.2440147399902344\n",
            "loss after epoch 44 is 3.2527270317077637\n",
            " accuracy in epoch 45 after steps 0 is: 0.09882000088691711\n",
            " Loss for main model3 in epoch 45 after steps 0 is: 3.343745708465576\n",
            "loss after epoch 45 is 2.960672378540039\n",
            " accuracy in epoch 46 after steps 0 is: 0.09848000109195709\n",
            " Loss for main model3 in epoch 46 after steps 0 is: 2.6665241718292236\n",
            "loss after epoch 46 is 2.918888568878174\n",
            " accuracy in epoch 47 after steps 0 is: 0.09777999669313431\n",
            " Loss for main model3 in epoch 47 after steps 0 is: 2.7053349018096924\n",
            "loss after epoch 47 is 2.6150527000427246\n",
            " accuracy in epoch 48 after steps 0 is: 0.10047999769449234\n",
            " Loss for main model3 in epoch 48 after steps 0 is: 2.855363368988037\n",
            "loss after epoch 48 is 2.827603816986084\n",
            " accuracy in epoch 49 after steps 0 is: 0.09994000196456909\n",
            " Loss for main model3 in epoch 49 after steps 0 is: 2.6109561920166016\n",
            "loss after epoch 49 is 2.6801233291625977\n",
            "final training accuracy is 0.0972890630364418\n",
            "test accuracy is 0.10189999639987946\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxxBxZjlFxSX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}